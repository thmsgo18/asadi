from typing import Any
from pydantic import Field
from decouple import config
import time, requests
from llama_index.core.llms import (
    CustomLLM,
    CompletionResponse,
    CompletionResponseGen,
    LLMMetadata,
)
from llama_index.core.llms.callbacks import llm_completion_callback
from requests.exceptions import RequestException

"""
Module d'interface avec l'API Pleiade LLM.

Ce module permet de cr√©er un type PleiadeLLM compatible avec LlamaIndex et fournit
des fonctions pour effectuer des requ√™tes au mod√®le de langage Pleiade.
"""

class PleiadeLLM(CustomLLM):
    """
    Classe d'interface avec l'API Pleiade compatible avec LlamaIndex.
    
    Cette classe permet d'utiliser le mod√®le de langage Pleiade avec l'√©cosyst√®me LlamaIndex
    en impl√©mentant les m√©thodes requises par l'interface CustomLLM.
    
    Attributes:
        api_key (str): Cl√© d'API pour l'authentification avec le service Pleiade.
        model_name (str): Nom du mod√®le √† utiliser (par d√©faut "llama3.3:latest").
        url (str): URL de l'endpoint API Pleiade.
        context_window (int): Taille de la fen√™tre de contexte en tokens.
        num_output (int): Nombre maximum de tokens en sortie.
    """
    api_key: str = Field(...)
    model_name: str = "llama3.3:latest"
    url: str = "https://pleiade.mi.parisdescartes.fr/api/chat/completions"
    context_window: int = 4096
    num_output: int = 512

    @property
    def metadata(self) -> LLMMetadata:
        """
        Fournit les m√©tadonn√©es du mod√®le LLM.
        
        Returns:
            LLMMetadata: Objet contenant les m√©tadonn√©es du mod√®le (taille de fen√™tre, capacit√©s, etc.).
        """
        return LLMMetadata(
            context_window=self.context_window,
            num_output=self.num_output,
            model_name=self.model_name,
            is_chat_model=True,
        )

    @llm_completion_callback()
    def complete(self, prompt: str, **kwargs: Any) -> CompletionResponse:
        """
        Envoie une requ√™te de compl√©tion au mod√®le Pleiade et retourne la r√©ponse.
        
        Args:
            prompt (str): Le texte d'entr√©e √† compl√©ter par le mod√®le.
            **kwargs (Any): Arguments suppl√©mentaires pour la requ√™te.
            
        Returns:
            CompletionResponse: R√©ponse format√©e contenant le texte g√©n√©r√©.
            
        Raises:
            Exception: Si la requ√™te √©choue ou si le statut de r√©ponse n'est pas 200.
        """
        headers = {
            "Authorization": f"Bearer sk-{self.api_key}",
            "Content-Type": "application/json"
        }

        data = {
            "model": self.model_name,
            "messages": [
                {"role": "system", "content": "Tu es un assistant intelligent qui r√©pond √† des questions administratives uniquement √† partir des informations fournies. Tu fournis aussi la source de tes r√©ponses, et tu pr√©cises quand tu ne peux pas r√©pondre."},
                {"role": "user", "content": prompt}
            ],
            "temperature": 0.7
        }

        start = time.time()
        try:
            response = requests.post(self.url, headers=headers, json=data, timeout=60)
        except RequestException as e:
            print(f"Erreur requ√™te LLM : {e}")
            raise Exception(f"Erreur requ√™te LLM : {e}")
        end = time.time()

        print(f"‚è± Temps de r√©ponse : {end - start:.2f} secondes")

        if response.status_code == 200:
            content = response.json()["choices"][0]["message"]["content"]
            return CompletionResponse(text=content)
        else:
            raise Exception(f"Erreur {response.status_code} : {response.text}")

    @llm_completion_callback()
    def stream_complete(self, prompt: str, **kwargs: Any) -> CompletionResponseGen:
        """
        M√©thode de streaming de compl√©tion (non impl√©ment√©e pour Pleiade).
        
        Args:
            prompt (str): Le texte d'entr√©e √† compl√©ter par le mod√®le.
            **kwargs (Any): Arguments suppl√©mentaires pour la requ√™te.
            
        Returns:
            CompletionResponseGen: G√©n√©rateur de r√©ponse pour le streaming.
            
        Raises:
            NotImplementedError: Cette fonctionnalit√© n'est pas support√©e par l'API Pleiade.
        """
        raise NotImplementedError("Le mod√®le Pleiade ne supporte pas le streaming.")


def reponseAssistant(results, query, history_messages=None):
    """
    G√©n√®re une r√©ponse de l'assistant bas√©e sur les r√©sultats de recherche et la requ√™te.
    
    Utilise l'API Pleiade pour g√©n√©rer une r√©ponse contextuelle bas√©e sur les r√©sultats
    fournis et l'historique de conversation optionnel.
    
    Args:
        results (str): R√©sultats de recherche ou contexte √† utiliser pour la r√©ponse.
        query (str): Question ou requ√™te de l'utilisateur.
        history_messages (list, optional): Historique des messages pr√©c√©dents. Defaults to None.
        
    Returns:
        str: R√©ponse g√©n√©r√©e par le mod√®le ou None en cas d'erreur.
    """
    url = "https://pleiade.mi.parisdescartes.fr/api/chat/completions"
    headers = {
        "Authorization": f"Bearer sk-{config('API_KEY')}",
        "Content-Type": "application/json"
    }

    system_prompt = """
    Tu es un assistant administratif et juridique qui r√©pond **uniquement** √† partir des extraits fournis. Ne compl√®te rien avec tes connaissances.

    Structure imp√©rative de ta r√©ponse :

    1. üßæ **Extrait(s) utile(s)** : Cite litt√©ralement l'extrait du ou des documents. Indique syst√©matiquement le nom du fichier source entre crochets, dans ce format : [[NomDuFichier.pdf]]

    2. ‚úÖ **R√©ponse finale** : Donne une r√©ponse claire, synth√©tique et conforme aux extraits.

    3. ‚ùì **Si l'information ne figure pas** dans les extraits : r√©ponds uniquement "Je n‚Äôai pas suffisamment d‚Äôinformations dans les sources." sans ajouter d‚Äôexplication ou d‚Äôinvention.

    Ne commente jamais la source en langage naturel. Ne fais aucune inf√©rence non justifi√©e. Ne cite jamais de texte qui n‚Äôest pas dans les extraits.
    """

  

    # On construit la liste des messages
    messages = [
        {"role": "system", "content": system_prompt},
    ]
    if history_messages:
        messages += history_messages

    # Enfin, le nouveau prompt utilisateur, avec le contexte RAG
    messages.append({
        "role": "user",
        "content": f"context :{results}, question : {query} ?"
    })

    print(messages)

    payload = {
        "model": "llama3.3:latest",
        "messages": messages,
        "temperature": 0.3
    }

    start_time = time.time()
    try:
        response = requests.post(url, headers=headers, json=payload, timeout=60)
    except RequestException as e:
        print(f"Erreur requ√™te LLM : {e}")
        return None
    end_time = time.time()

    if response.status_code != 200:
        print(f"Erreur {response.status_code} : {response.text}")
        return None

    try:
        reponse_ia = response.json()["choices"][0]["message"]["content"]
    except (KeyError, IndexError):
        print("Erreur : Structure de r√©ponse inattendue")
        return None

    print(f"Temps de r√©ponse : {end_time - start_time:.2f} s")
    print(results)
    return reponse_ia






def resumeDocument(document):
    url = "https://pleiade.mi.parisdescartes.fr/api/chat/completions"
    headers = {
        "Authorization": f"Bearer sk-{config('API_KEY')}",
        "Content-Type": "application/json"
    }

    system_prompt = """
    Tu es un assistant expert en analyse de documents administratifs.

Ta mission est de lire un document administratif complet et d‚Äôen produire un **r√©sum√© clair, structur√© et synth√©tique** qui pourra servir de chunk de r√©sum√© dans un syst√®me RAG (Retrieval-Augmented Generation).

Ce r√©sum√© doit :
- √ätre **compr√©hensible par une personne non sp√©cialiste**
- R√©sumer en **un seul paragraphe (80 √† 150 mots)** les **objectifs du document, les b√©n√©ficiaires concern√©s, les conditions ou crit√®res mentionn√©s, les d√©marches √† suivre et les √©ventuelles dates cl√©s**
- √ätre **fid√®le au contenu du document**, sans interpr√©tation ni ajout
- √ätre **utile pour r√©pondre √† des requ√™tes g√©n√©rales** telles que : ¬´ √Ä quoi sert ce document ? ¬ª, ¬´ Qui est concern√© ? ¬ª, ¬´ Quelles sont les √©tapes de la proc√©dure ? ¬ª
- Ne pas commencer par ¬´ Ce document explique‚Ä¶ ¬ª ou ¬´ Le pr√©sent document‚Ä¶ ¬ª ‚Üí formule directe, factuelle
- √âviter les abr√©viations, sigles ou formulations trop techniques si une paraphrase claire est possible

Ne cite pas les annexes sauf si elles contiennent des informations essentielles √† la compr√©hension du document.

R√©dige un paragraphe unique, informatif, fluide et compact, pr√™t √† √™tre int√©gr√© dans une base de donn√©es vectorielle.

    """
  

    # On construit la liste des messages
    messages = [
        {"role": "system", "content": system_prompt},
    ]
 
    messages.append({
        "role": "user",
        "content": f"Document  : {document}"
    })

    payload = {
        "model": "llama3.3:latest",
        "messages": messages,
        "temperature": 0.3
    }

    start_time = time.time()
    try:
        response = requests.post(url, headers=headers, json=payload, timeout=60)
    except RequestException as e:
        print(f"Erreur requ√™te LLM : {e}")
        return None
    end_time = time.time()

    if response.status_code != 200:
        print(f"Erreur {response.status_code} : {response.text}")
        return None

    try:
        reponse_ia = response.json()["choices"][0]["message"]["content"]
    except (KeyError, IndexError):
        print("Erreur : Structure de r√©ponse inattendue")
        return None

    print(f"Temps de r√©ponse : {end_time - start_time:.2f} s")
    return reponse_ia






def generate_prompt_title(first_question: str) -> str:
    """
    G√©n√®re un titre court pour une conversation bas√© sur la premi√®re question.
    
    Appelle le LLM Pleiade pour produire un titre concis et pertinent √† partir
    de la premi√®re question d'une conversation.
    
    Args:
        first_question (str): La premi√®re question pos√©e par l'utilisateur.
        
    Returns:
        str: Un titre court g√©n√©r√© pour la conversation ou "Nouveau prompt" en cas d'√©chec.
    """
    url = "https://pleiade.mi.parisdescartes.fr/api/chat/completions"
    headers = {
        "Authorization": f"Bearer sk-{config('API_KEY')}",
        "Content-Type": "application/json",
    }
    data = {
        "model": "llama3.3:latest",
        "messages": [
            {"role": "system","content": ("Tu es un assistant qui g√©n√®re un titre court et pertinent pour " "une question administrative.")},
            {"role": "user","content": f"Propose-moi un titre bref pour cette question: ¬´{first_question}¬ª" "‚Äî et **sans** guillemets autour du titre."}
        ],
        "temperature": 0.3,
        # Possibilit√© de limiter le nombre de tokens pour rester sur un titre
        "max_tokens": 20
    }
    start = time.time()
    try:
        resp = requests.post(url, headers=headers, json=data, timeout=60)
    except RequestException as e:
        print(f"Erreur requ√™te LLM : {e}")
        return "Nouveau prompt"
    elapsed = time.time() - start

    if resp.status_code == 200:
        try:
            title = resp.json()["choices"][0]["message"]["content"].strip()
            return title
        except (KeyError, IndexError):
            pass
    # fallback en cas d'erreur
    return "Nouveau prompt"


def generate_quiz_title(theme: str) -> str:
    """
    G√©n√®re un titre attrayant pour un quiz bas√© sur son th√®me.
    
    Appelle le LLM Pleiade pour cr√©er un titre accrocheur et original
    pour un quiz √©ducatif sur le th√®me sp√©cifi√©.
    
    Args:
        theme (str): Le th√®me du quiz pour lequel g√©n√©rer un titre.
        
    Returns:
        str: Un titre accrocheur pour le quiz ou "Quiz sur {theme}" en cas d'√©chec.
    """
    url = "https://pleiade.mi.parisdescartes.fr/api/chat/completions"
    headers = {
        "Authorization": f"Bearer sk-{config('API_KEY')}",
        "Content-Type": "application/json",
    }
    data = {
        "model": "llama3.3:latest",
        "messages": [
            {"role": "system","content": "Tu es un assistant qui g√©n√®re des titres cr√©atifs et engageants pour des quiz √©ducatifs."},
            {"role": "user","content": f"Cr√©e un titre accrocheur et original pour un quiz sur le th√®me: '{theme}'. " 
                                    "Le titre doit √™tre court (maximum 6 mots), captivant et donner envie de participer. " 
                                    "Ne mets pas de guillemets autour du titre et n'utilise pas l'expression 'Quiz sur...'"}
        ],
        "temperature": 0.7,  # Plus de cr√©ativit√© pour les titres de quiz
        "max_tokens": 15
    }
    start = time.time()
    try:
        resp = requests.post(url, headers=headers, json=data, timeout=60)
    except RequestException as e:
        print(f"Erreur requ√™te LLM : {e}")
        return f"Quiz sur {theme}"
    elapsed = time.time() - start

    if resp.status_code == 200:
        try:
            title = resp.json()["choices"][0]["message"]["content"].strip()
            # S'assurer que le titre n'est pas trop long
            if len(title) > 50:
                title = title[:47] + "..."
            return title
        except (KeyError, IndexError):
            pass
    # fallback en cas d'erreur
    return f"Quiz sur {theme}"


def pimp(query):
    """
    Am√©liore une requ√™te utilisateur pour obtenir de meilleurs r√©sultats.
    
    Utilise l'API Pleiade pour reformuler et enrichir une requ√™te utilisateur
    afin d'am√©liorer les r√©sultats de recherche.
    
    Args:
        query (str): La requ√™te utilisateur √† am√©liorer.
        
    Returns:
        str: La requ√™te am√©lior√©e ou la requ√™te originale en cas d'√©chec.
    """
    url = "https://pleiade.mi.parisdescartes.fr/api/chat/completions"
    headers = {
        "Authorization": f"Bearer sk-{config('API_KEY')}",
        "Content-Type": "application/json"
    }
    data = {
        "model": "llama3.3:latest",
        "messages": [
            {"role": "system", "content": "Tu es charg√© de retourner un prompt plus ad√©quat √† une recherche rag, sans pour autant d√©naturer son sens pour que l'utilisateur ai sa r√©ponse demand√©. √Ä titre d'exemple tu vas √©liminer les fautes d'orthographes et de syntaxe. Retourne le prompt modifi√© uniquement et n'ajoute pas de contexte,juste le prompt. Pr√©cise dans quels documents tu as cherch√© tes informations "},
            {"role": "user", "content": f"question : {query} ?"}
        ],
        "temperature": 0.7
    }
    start_time = time.time()
    try:
        response = requests.post(url, headers=headers, json=data, timeout=60)
    except RequestException as e:
        print(f"Erreur requ√™te LLM : {e}")
        return 
    end_time = time.time()

    if response.status_code == 200:
        try:
            # Extraire la r√©ponse de l'IA
            reponse_ia = response.json()["choices"][0]["message"]["content"]
        except (KeyError, IndexError):
            print("Erreur : Structure de r√©ponse inattendue")
    else:
        print(f"Erreur {response.status_code} : {response.text}")
        return 
    
    print(f"Temps de r√©ponse : {end_time - start_time:.2f} secondes")
    return reponse_ia
